prompts:
  transcript_to_verbatim:
    content: |
      #Background
      你是一个非常专业的文字编辑，用户会给你提供一个精彩活动的录音稿，里面有一些录音的谬误和语气词，用户希望将其转化成一个富有信息和启发性的逐字稿用于后续回顾和反复学习。
      #goal
      - 你需要理解录音稿的意思。并且将其重构流畅易懂的书面语言
      - 你需要将录音稿*从头到尾*转化为一篇逻辑清晰、易懂的书面文稿。以原本的第一人称方式进行。
      - 以不同发言人以及其相应发言分隔断
      - 可以加入一些表示内容主题的文字
      - 若语言为英文，将内容转化为中文作为主要语言进行输出
      - 不可以遗漏任何观点、内容、信息
      - 直接输出转写的文字，不需要输出类似“好的，下面是转写之后的文章”之类的文字
      #constraint：
      - 你生成的必须是从头到尾完整的文字稿，绝对不可以以总结的方式省略内容！！！
      - 由于我需要展示录音稿中所有的信息，你不可以省略录音稿中和主题相关的任何内容！
      - 使用中文回复


  verbatim_to_json:
    content: |
      你是一个文本编辑大师，用户会给你一篇文章，你需要根据用户给你的文章，输出以下的json格式的格式化数据，直接输出json格式，不要输出其他内容：
      {
          "guest_name": "王鸿钰",
          "guest_intro": "这一期的对话栏目，我们邀请到了王鸿钰师兄（1811/1805），他在高效深度学习模型领域做出了卓越贡献，例如DeepNet、Magneto、BitNet系列研究。其中BitNet系列模型成功将庞大的大语言模型压缩至前所未有的低比特位，使得在消费级硬件上运行尖端AI模型成为可能，极大地推动了大模型技术的的普及和应用，成果获得业界的广泛关注与"新智元"、"量子位"、Forbes、VentureBeat等顶尖科技资讯平台专文报道。王鸿钰深度参与了首个1-bit大规模语言模型BitNet b1.58 2B的训练和开源，模型发布一月内在Huggingface下载量超过12万次。推理框架BitNet.cpp在GitHub发布首周获得超过1万星。",
          "interviewer": "金雨润、冯文俊",
          "proofreader": "占一、苏启晟",
          "word_count": "9411",
          "reading_time": "21",
          "topics": [
              "如何迈出科研第一步",
              "科研入门经历与挑战",
              "选择导师的关键",
              "研究方向的选择与确定",
              "如何判断某条方向的前途"
          ],
          "main_sections": [
              {
                  "id": "01",
                  "title": "溯源科大",
                  "sub_sections": [
                      {
                          "subtitle": "成长与选择",
                          "question": "师兄可以简单介绍下您的大学生涯嘛？",
                          "answer": "我刚开始是在工程科学学院就读。但我从初中开始就对计算机和人工智能领域抱有非常浓厚的兴趣，所以在进入科大时，我就有非常强烈的转专业的意愿，最后成功在二年级时转到了计算机学院。非常幸运的是，我在大三的时候参加了微软亚洲研究院的一个选拔项目，名额主要分为两类，一类是中科大和微软联合培养项目，另一类是一年期的科研实习。我拿到的是后者。大四一年，我在MSRA韦福如老师和马树铭老师的指导下进行基础模型架构设计的科研。当时我并没有直接拿到中科大和MSRA的联培名额。不过，因为我在MSRA实习的一年里确实做出了一些还不错的成果，所以之后在我去中科院读博期间，也再次获得了进入MSRA实习的机会。"
                      },
                      {
                          "subtitle": "成长与选择",
                          "question": "回顾您的大学生涯,是怎样的经历促使您最终选择了AI这一方向?",
                          "answer": "我选择AI，更多还是源于从小就对AI本身怀有的强烈兴趣。实际上，我的专业方向选择其实每次调整幅度都比较大，在科大期间接触的方向和我目前从事的具体领域（比如现在比较热门的大语言模型）有很大不同。"
                      },
                      {
                          "subtitle": "MSRA的科研经历",
                          "question": "在您成长的过程中,有哪些经历或人物对您的价值观和性格形成产生了深远影响?",
                          "answer": "我的科研起步基本上都是在MSRA完成的。我大四之前几乎没有系统地接触过任何与我现在研究领域直接相关的东西，可能除了大三时学过一门《人工智能基础》课程之外，就没有再深入涉猎其他相关领域了。"
                      },
                      {
                          "subtitle": "AI工具之我见",
                          "question": "当前许多大学生在日常学习和编程实践中广泛使用AI工具,您如何看待这一现象?",
                          "answer": "我对学生使用AI工具持比较开放的态度。比如ChatGPT写的代码，很多时候可能比我自己写的还好。"
                      }
                  ]
              },
              {
                  "id": "02",
                  "title": "探索与突破",
                  "sub_sections": [
                      {
                          "subtitle": "科研的挑战与收获",
                          "question": "您的科研入门经历时什么样的?",
                          "answer": "我做的第一个工作是DeepNet，这个工作首次将Transformer模型扩展到了1000层，解决了Transformer在深度训练过程中的稳定性问题。之后，我们又在这个工作的基础上做了一个通用的模型Magneto，将其扩展到视觉预训练、语音识别和多模态领域形成了一个通用的Transformer架构。这个架构也成为了之后BitNet系列的基础。"
                      },
                      {
                          "subtitle": "科研的挑战与收获",
                          "question": "回顾您的大学和科研生涯,您是如何在不同机会与方向之间做出选择和取舍的?",
                          "answer": "我的个人经历可能不具备太大的普适性，我运气确实太好了。在很多关键节点我并没有刻意去规划，但都恰好走对了。如果完全按照我当时的成长路径来复制，我觉得是很难重现的。"
                      },
                      {
                          "subtitle": "科研的挑战与收获",
                          "question": "在科研生涯早期,您曾遇到过哪些关键的挑战?您是如何应对并克服它们的?",
                          "answer": "我的科研之路还是比较顺利的。我第一个项目DeepNet是在MSRA开始的，当时我对很多东西都还不懂。但我投入的时间也确实非常多，基本上每天都是高强度地投入到科研中。我们组会根据每个学生的特质去设置和分配项目，这也尽可能发挥了我的长处。我当时跟导师说我编程能力相对弱一些，但数学能力还可以，于是他就给了一个比较偏数学理论的项目。"
                      },
                      {
                          "subtitle": "科研的挑战与收获",
                          "question": "您认为学生如何有效地选择适合自己的研究环境与导师?",
                          "answer": "选择适合自己的研究环境和导师是一个很难的问题。如果大家是为了给自己的履历增加一份实习经历，首先要考虑实验室的资源，比如算力是否充足，是否愿意给实习生分配算力。"
                      },
                      {
                          "subtitle": "BitNet系列高效模型的研究",
                          "question": "您深度参与了BitNet系列高效模型的研究,能否分享一下BitNet背后的核心思想和创新点?",
                          "answer": "模型量化是现在提高大模型推理效率的一类比较常用的方法。我们现在的大模型在训练时，一般采用BF16的精度，模型的每个参数用16个比特来表示。这样一个百亿参数的模型，乘以16比特，占用的存储空间就非常大。"
                      },
                      {
                          "subtitle": "BitNet系列高效模型的研究",
                          "question": "BitNet 系列模型在发布后引发了巨大的关注,有没有特别难忘的瞬间或感触深刻的反馈?",
                          "answer": "BitNet b1.58是我们24年3月份在arXiv上发布的。当时在Hugging Face和X等社交媒体上一经推出就非常火。直到现在，Hugging Face上每日热门论文的榜单，我们BitNet的Upvote应该还是第一名，我记得好像有600多，而第二名可能只有三百多。这也反映出大家对于轻量化大模型确实有非常迫切的需求。去年其实也有很多初创公司和大厂邀请我去帮助他们做相关的技术。但我们觉得BitNet的潜力其实还没有完全发挥出来，因为我们当时做的还只是一个比较初期的版本。最近我们还做了一个结合1.58-bit权重和4-bit激活的大语言模型，其推理性能基本上达到了现有芯片硬件能支持的极限。如果算法上想再追求进一步提升，可能就需要全新的硬件芯片设计了。后续我们还会更加关注这些技术的逐步落地，把近两年的一些研究成果应用到实际中，去训练真正可用的、高效的语言模型。"
                      }
                  ]
              },
              {
                  "id": "03",
                  "title": "洞见与前瞻",
                  "sub_sections": [
                      {
                          "subtitle": "洞见与前瞻",
                          "question": "大模型量化压缩(如BitNet)与多模态推理是您主要关注的研究方向,您认为这两个方向未来的互动将产生怎样的创新机会,有哪些潜在的应用场景?",
                          "answer": "我觉得多模态技术，尤其是在理解层面，目前已经有点接近饱和了。多模态现在渐渐地有点变成大语言模型的一个下游任务。也就是说，一个算法首先在大语言模型上得到验证和应用，效果比较好之后，能够比较自然地迁移到多模态领域。它现在更像是大语言模型能力的一种延伸，很多能够在大语言模型上奏效的方法，也能够应用到多模态理解这个任务上，两者已经高度重合了。"
                      },
                      {
                          "subtitle": "洞见与前瞻",
                          "question": "AI领域快速迭代变化,不确定性很强,您认为研究者应如何有效地把握科研方向并抓住机遇? 哪些核心能力在这样的环境中变得尤为重要?",
                          "answer": "我现在感觉AI领域的技术迭代真的是一年一代。LLM真正火起来是22年10月ChatGPT刚发布的时候。到了2023年4月份，GPT-4发布。在23年初的时候，GPT-4的API调用还相当昂贵，那时你很难想象会出现一个GPT-4级别并且完全开源的模型。当时大家普遍觉得和OpenAI的技术差距非常大，短时间内可能都无法追赶。"
                      },
                      {
                          "subtitle": "洞见与前瞻",
                          "question": "在您看来,当下能够在科研道路上取得显著成就的人通常具备哪些共同特点?",
                          "answer": "我觉得，要么就是运气特别好，要么就是规划做得特别好，这两者至少得占一个。"
                      },
                      {
                          "subtitle": "洞见与前瞻",
                          "question": "您如何看待个人努力与时代机遇之间的关系?",
                          "answer": "我觉得大家也不需要那么"卷"，放宽心态。只要你能够勇敢地迈出自己的舒适圈，就已经比大部分人要领先一步了。至于最终成果如何，你的论文能不能发表，甚至这个方向能不能火起来，某种程度上是"时代的选择"，而不完全是个人努力能决定的。在正确的时间点做正确的事情，可能比单纯的埋头苦干更重要。"
                      },
                      {
                          "subtitle": "洞见与前瞻",
                          "question": "当前企业在AI 大模型研究领域的资源优势明显,前沿研究重心逐渐从高校向企业转移,您如何看待这一现象?",
                          "answer": "我觉得现在工业界正在独立地培养自己的人才梯队。大家可能也发现了，在LLM领域，真正做出有影响力工作的团队，成员都非常年轻。"
                      },
                      {
                          "subtitle": "洞见与前瞻",
                          "question": "对于科大求学、有志于投身AI研究的学弟学妹,您有什么特别想分享的建议?",
                          "answer": "关于AI科研，我还是想反复强调之前提到的那个问题：对于本科生，尤其是科大的本科生，我觉得第一个要克服的，往往不是科研资源的问题，而是心理障碍，比如是否可以跳出卷GPA的赛道去多搞搞科研。"
                      }
                  ]
              }
          ]
      }